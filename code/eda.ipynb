{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create our work dataset from two other ones. \n",
    "\n",
    "<hr>\n",
    "\n",
    "Domain dataset, which contains species names from GBIF site.\n",
    "\n",
    "Out-of-domain dataset, with general knowlegde texts in plain-text files. For this example, we use abstracts from the PLOS site, but feel free to replace it with another files.\n",
    "\n",
    "Then, we draw some insights to create our model and the text handling. Such as, size of the samples, quantity, stategy, etc.\n",
    "\n",
    "We also use the <a href=\"https://bitbucket.org/conabio_cmd/conabio_ml_text\">CONABIO ML Text</a> library to template an end-to-end pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pydash\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use the CONABIO_ML library code always remember to update your PYTHONPATH env variable with\n",
    "# export PYTHONPATH=`pwd`:`pwd`/conabio_ml_text/conabio_ml:`pwd`/conabio_ml_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conabio_ml_text.datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the dataset using two types of samples: species names and common knowledge words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_path = Path(\"dataset\")\n",
    "\n",
    "d_dataset_path = base_dataset_path / \"species.txt\"\n",
    "ood_dataset_path = base_dataset_path / \"text_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain specific dataset is a plain text file (<i>We assume it located at `base_dataset_path / \"species.txt\"` </i>) that contains taxonomic trees of species, with the following format:\n",
    "\n",
    "`species_parent, species, â€¦ `.\n",
    "\n",
    "<hr>\n",
    "\n",
    "We obtained this dataset from the <a href=\"https://www.gbif.org/developer/species\">GBIF species API</a> using `Animalia` as a root taxonomic tree. \n",
    "\n",
    "You can gather the json representations of the taxonomic tree, and then convert it to plain text, according to your needs using the `dataset_builder.py` script.\n",
    "\n",
    "The dataset for this example is available to download, from <a href=\"https://tctp-datasets.s3.us-south.cloud-object-storage.appdomain.cloud/species.txt\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_names = \"\"\n",
    "with open(d_dataset_path) as _f:\n",
    "    species_names = _f.read()\n",
    "    \n",
    "species_trees = species_names.split(\"\\n\")\n",
    "species = set(pydash.chain(species_trees)\\\n",
    "              .filter(lambda x: len(x) > 0)\\\n",
    "              .map(lambda x: x.lower().split(\",\"))\\\n",
    "              .flatten()\\\n",
    "              .map(lambda x: x.replace(\" \", \"_\"))\\\n",
    "              .value())\n",
    "\n",
    "domain_dataset = pd.DataFrame(list(species), columns = [\"item\"])\n",
    "domain_dataset[\"label\"] = \"species\"\n",
    "domain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domain_dataset[\"item\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we draw some basic insights of the d-dataset (domain dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_lengths = domain_dataset[\"item\"].apply(lambda x: len(x))\n",
    "species_words = domain_dataset[\"item\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "MIN_CHAR_SIZE = int(np.min(species_lengths))\n",
    "pprint(f\"Mean char size of species: {np.mean(species_lengths)}.\")\n",
    "pprint(f\"Max char size of species: {np.max(species_lengths)}.\")\n",
    "pprint(f\"Min char size of species: {MIN_CHAR_SIZE}.\")\n",
    "\n",
    "MEAN_SPECIES_WORDS = int(np.mean(species_words))\n",
    "pprint(f\"Mean word size of species: {MEAN_SPECIES_WORDS}. Max word size of species: {np.max(species_words)}\")\n",
    "\n",
    "\n",
    "\n",
    "pprint(f\"Dataset size: {len(domain_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, at word level. We have the number of unique tokens in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_level = pydash.chain(domain_dataset[\"item\"])\\\n",
    "            .map(lambda x: set(x.split()))\\\n",
    "            .reduce(lambda x, y: x.union(y), set())\\\n",
    "            .value()\n",
    "\n",
    "pprint(f\"And we have {len(word_level)} unique species words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "For the non-domain dataset, we use a set of `1000` abstracts with subject `health sciences` obtained from the  <a href=\"https://plos.org/\">PLOS site</a>.  Gathered using the <a href=\"https://github.com/thecopy-and-thepaste/qtod\">qtod module</a>.\n",
    "\n",
    "You can use your own plain text files or just download the files we are working with from <a href=\"https://tctp-datasets.s3.us-south.cloud-object-storage.appdomain.cloud/clsspec_text_files.zip\">HERE</a>, and extract it. We assume the path for the files in `base_dataset_path / \"text_files\"`.\n",
    "\n",
    "Then, we just extract 1-3 grams taking care that samples don't repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_numbers = re.compile('^[-+]?[\\d.]+(?:e-?\\d+)?$')\n",
    "\n",
    "def ood_preproc(item_path:str):\n",
    "    try:\n",
    "        with open (item_path, mode=\"r\", encoding='utf-8') as _f:\n",
    "            item = _f.read()\n",
    "\n",
    "        tokens = []\n",
    "        # We only care to remove hyperlink, puntuation, and numbers.\n",
    "        item = item.lower()\n",
    "\n",
    "        item = item.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "        for token in item.split():               \n",
    "            if re.findall(re_numbers, token):\n",
    "                continue\n",
    "\n",
    "            tokens.append(token)\n",
    "\n",
    "        ix = 0\n",
    "        while ix < len(tokens):\n",
    "            step = random.randint(1, 3)\n",
    "\n",
    "            yield \"_\".join(tokens[ix: ix+step])\n",
    "            ix += step\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood = Dataset.from_folder(source_path=ood_dataset_path,\n",
    "                          extensions=[\"txt\"],\n",
    "                          recursive=False,\n",
    "                          label_by_folder_name=True,\n",
    "                          split_by_folder=False,\n",
    "                          include_id=False,\n",
    "                          item_reader = ood_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_items = ood.data[\"item\"].unique()\n",
    "ood_dataset = pd.DataFrame(ood_items, columns=[\"item\"])\n",
    "ixs = ood_dataset.apply(lambda x: len(x[\"item\"]) > MIN_CHAR_SIZE, axis = 1)\n",
    "\n",
    "ood_dataset = ood_dataset.loc[ixs]\n",
    "ood_dataset[\"label\"] = \"non_species\"\n",
    "ood_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, just drop as a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([ood_dataset, domain_dataset])\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset.to_csv(base_dataset_path / \"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset), len(ood_dataset[\"item\"].unique()) + len(domain_dataset[\"item\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"{len(dataset)} samples\")\n",
    "pprint(f'Species samples: {len(dataset[dataset[\"label\"] == \"species\"])}')\n",
    "pprint(f'Non-Species samples: {len(dataset[dataset[\"label\"] == \"non_species\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(base_dataset_path / \"dataset.csv\")\n",
    "ds.reporter(\".\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
