{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we perform the training process and the evaluation.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The process consists on the following stages:\n",
    "\n",
    "1. Load the dataset produced in the `eda` notebook.\n",
    "2. Generate n-char samples, then, add noise to the samples to simulate noisy process like OCR detection.\n",
    "3. Create the model, and the config model (in case you want to use `pipeline.py`), and train it.\n",
    "4. Make a predictions of the dataset and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to update the PYTHON_PATH to\n",
    "# export PYTHONPATH=`pwd`:`pwd`/conabio_ml_text/conabio_ml:`pwd`/conabio_ml_text\n",
    "\n",
    "from conabio_ml_text.datasets.dataset import Dataset, Partitions\n",
    "from conabio_ml_text.preprocessing.preprocessing import Tokens, PreProcessing\n",
    "from conabio_ml_text.preprocessing.transform import Transform\n",
    "\n",
    "from conabio_ml_text.trainers.bcknds.tfkeras import TFKerasTrainer, TFKerasTrainerConfig\n",
    "from conabio_ml_text.trainers.bcknds.tfkeras import CHECKPOINT_CALLBACK, TENSORBOARD_CALLBACK\n",
    "\n",
    "from conabio_ml_text.utils.constraints import TransformRepresentations as TR\n",
    "\n",
    "from conabio_ml.evaluator.generic.evaluator import Evaluator, Metrics\n",
    "\n",
    "from conabio_ml.utils.logger import get_logger, debugger\n",
    "\n",
    "from utils import nchars, datagen\n",
    "from model import LSTMModel\n",
    "\n",
    "log = get_logger(__name__)\n",
    "debug = debugger.debug\n",
    "\n",
    "Tokens.UNK_TOKEN = Tokens.UNK_TOKEN * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable this to see the `debug` messages\n",
    "# debugger.create(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = Path(\"dataset/dataset.csv\")\n",
    "results_path = Path(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset from the `csv` file and perform a split in `[train, validation, test]` partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 16:58:05,000 [conabio_ml.datasets.dataset] [INFO ]  Creating dataset from CSV file: /Users/rrivera/Documents/m.ind/clsspec/code/dataset/dataset.csv\n",
      "2021-02-15 16:58:05,106 [conabio_ml.datasets.dataset] [DEBUG]  Columns in dataset: Index(['Unnamed: 0', 'item', 'label'], dtype='object')\n",
      "2021-02-15 16:58:05,108 [conabio_ml.datasets.dataset] [DEBUG]  Creating dataset with 81069 registers\n",
      "2021-02-15 16:58:05,130 [conabio_ml.datasets.dataset] [DEBUG]  Dataset with ordinal labels\n",
      "2021-02-15 16:58:05,138 [conabio_ml.datasets.dataset] [DEBUG]  2 categories in dataset\n",
      "2021-02-15 16:58:05,139 [conabio_ml.datasets.dataset] [INFO ]  Assigning labelmap with [{0: 'non_species'}, {1: 'species'}]\n",
      "2021-02-15 16:58:05,140 [conabio_ml.datasets.dataset] [WARNI]  Column partition is not present in the dataset, so it has not been partitioned\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_csv(dataset_filepath)\n",
    "dataset = Dataset.split(dataset,\n",
    "                        train_perc=0.8,\n",
    "                        test_perc=0.1,\n",
    "                        val_perc=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8108, 8107)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.get_partition(\"test\")), len(dataset.get_partition(\"test\")[\"item\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training process, based on a `char-rnn`, we create samples of n-chars to capture insights.\n",
    "\n",
    "Note we use the property `build_vocab=True` to create the vocabulary accordding to the resulting dataset.Also, the `func_args` parameters are sent to the preprocess function, defined in `preprocess_fn=nchars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK][UNK][UNK]', 'sap', 'apo', 'pov']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sap apo pov ovi vir iru rus us_ s_i _is is_ s_a</td>\n",
       "      <td>non_species</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gen enu nus us_ s_o _of</td>\n",
       "      <td>non_species</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cal ali lic ici civ ivi vir iru rus use ses</td>\n",
       "      <td>non_species</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tha hat</td>\n",
       "      <td>non_species</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>are</td>\n",
       "      <td>non_species</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             item        label  \\\n",
       "0           0  sap apo pov ovi vir iru rus us_ s_i _is is_ s_a  non_species   \n",
       "1           1                          gen enu nus us_ s_o _of  non_species   \n",
       "2           2      cal ali lic ici civ ivi vir iru rus use ses  non_species   \n",
       "3           3                                          tha hat  non_species   \n",
       "4           4                                              are  non_species   \n",
       "\n",
       "    partition  \n",
       "0       train  \n",
       "1       train  \n",
       "2  validation  \n",
       "3       train  \n",
       "4       train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = np.max(dataset.data[\"item\"].apply(lambda x: len(x)))\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset = PreProcessing.preprocess(dataset,\n",
    "                                   build_vocab=True,\n",
    "                                   preprocess_args={\n",
    "                                       \"fields\": [\"item\"],\n",
    "                                       \"func_args\": {\n",
    "                                           \"pad_size\": -1,\n",
    "                                           \"nchar_size\": 3,\n",
    "                                           \"unk_token\": Tokens.UNK_TOKEN\n",
    "                                       }\n",
    "                                   },\n",
    "                                   preprocess_fn=nchars)\n",
    "# # By this moment the dataset is already processed and the vocab property exists\n",
    "# # Taken from TransformRepresentations constraints\n",
    "pprint(dataset.representations[TR.VOCAB][0:5])\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add a noise process. \n",
    "\n",
    "We produce 3 new noise samples from 1 real sample, 4 in total. With the following methodology:\n",
    "1. Real sample [nchar_1, nchar_2, â€¦, nchar_N, PAD_TOKEN]\n",
    "2. Two samples replacing 1 {nchar_1, nchar_N} to UNK_TOKEN\n",
    "3. One sample with both previously replaced `nchars``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Transform.as_data_generator(dataset,\n",
    "                                      vocab=None,\n",
    "                                      transform_args={\n",
    "                                          \"pad_length\": SAMPLE_SIZE,\n",
    "                                          \"unk_token\": Tokens.UNK_TOKEN,\n",
    "                                          \"batch_size\": BATCH_SIZE\n",
    "                                      },\n",
    "                                      data_generator=datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Be aware with batch size in model training, we are using the current batch_size in \n",
    "## `as_data_generator`, because it will produce samples of `[batch_size X 4 X max_len] (3D)`\n",
    "## instead of `[(batch_size X 4) X max_len] (2D)`\n",
    "#vocab = dataset.representations[TR.VOCAB]\n",
    "#gen = dataset.representations[\"data_generators\"][\"train\"]\n",
    "#sample_x, sample_y = next(gen())\n",
    "#[print([vocab[char] for char in sample]) for sample in sample_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trainer_config` contains parameters of the environment where the model will be trained. Like callbacks, and strategies (if there is one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REPRESENTATION = TR.DATA_GENERATORS\n",
    "trainer_config = TFKerasTrainerConfig.create(config={\n",
    "        \"strategy\": None,\n",
    "        \"callbacks\": {\n",
    "            CHECKPOINT_CALLBACK: {\n",
    "                \"filepath\": os.path.join(results_path, \"checkpoints\"),\n",
    "                \"save_best_only\": False\n",
    "            },\n",
    "            TENSORBOARD_CALLBACK: {\n",
    "                \"log_dir\": os.path.join(results_path, \"tb_logs\")\n",
    "            }}\n",
    "    })\n",
    "VOCAB_SIZE = len(dataset.representations[TR.VOCAB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a template of the `config`, in case you want to use it in `pipeline.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'code/dataset/dataset.csv',\n",
       " 'layers': {'input': {'T': 49},\n",
       "  'embedding': {'V': 11660, 'D': 100},\n",
       "  'lstm': {'M': 20},\n",
       "  'dense': {'K': 2}},\n",
       " 'params': {'initial_learning_rate': 0.0001,\n",
       "  'decay_steps': 200,\n",
       "  'batch_size': 16,\n",
       "  'epochs': 1}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset\": str(\"code\" / dataset_filepath),\n",
    "    \"layers\": {\n",
    "        \"input\": {\n",
    "            \"T\": int(SAMPLE_SIZE)\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"V\": VOCAB_SIZE,\n",
    "            \"D\": 100\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"M\": 20\n",
    "        },\n",
    "        \"dense\": {\n",
    "            \"K\": 2\n",
    "        }\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"initial_learning_rate\": 0.0001,\n",
    "        \"decay_steps\": 200,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/config.json\", mode=\"w\") as _f:\n",
    "    json.dump(config, _f, indent=4)\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Model definition.<br> <i>In case you change it, match config (or create a new one), with your model.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = config[\"layers\"]\n",
    "model = LSTMModel.create(model_config={        \n",
    "    \"CLASSIFIER\": {\n",
    "        \"layers\": {\n",
    "            \"input\": layers[\"input\"],\n",
    "            \"embedding\": layers[\"embedding\"],\n",
    "            \"lstm\": layers[\"lstm\"],\n",
    "            \"dense\": layers[\"dense\"]\n",
    "        }\n",
    "    }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all keyparts: `dataset`, `model`, `trainer_config`, to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocab\n",
      "_________________\n",
      "{'V': 11660, 'D': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 16:58:12,486 [conabio_ml_text.trainers.bcknds.tfkeras_models] [INFO ]  Starting fitting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16213/Unknown - 437s 27ms/step - loss: 0.0834 - accuracy: 0.9747\n",
      "Epoch 00001: saving model to results/checkpoints\n",
      "WARNING:tensorflow:From /Users/rrivera/Documents/m.ind/clsspec/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 17:05:50,454 [tensorflow  ] [WARNI]  From /Users/rrivera/Documents/m.ind/clsspec/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 17:05:51,684 [tensorflow  ] [INFO ]  Assets written to: results/checkpoints/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16213/16213 [==============================] - 455s 28ms/step - loss: 0.0834 - accuracy: 0.9747 - val_loss: 0.0348 - val_accuracy: 0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 17:05:51,908 [conabio_ml_text.trainers.bcknds.tfkeras_models] [INFO ]  Model fitting finished\n"
     ]
    }
   ],
   "source": [
    "lr = config[\"params\"][\"initial_learning_rate\"]\n",
    "epochs = config[\"params\"][\"epochs\"]\n",
    "trained_model = TFKerasTrainer.train(dataset=dataset,\n",
    "                                         model=model,\n",
    "                                         execution_config=trainer_config,\n",
    "                                         train_config={\n",
    "                                             \"CLASSIFIER\": {\n",
    "                                                 \"representation\": TRAIN_REPRESENTATION,\n",
    "                                                 'optimizer': tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                                                 'loss': tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                                 \"batch_size\": None,\n",
    "                                                 \"epochs\": epochs,\n",
    "                                                 \"metrics\": [\"accuracy\"]\n",
    "                                             }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate a new dataset with the predictions. \n",
    "\n",
    "We show the resulting dataset structure, we use the param `sparse_predictions=False`, by default it computes the `np.max` function over samples.\n",
    "\n",
    "<i>If other function is required, use the `pred_converter_fn` parameter</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 17:07:00,231 [conabio_ml.datasets.dataset] [DEBUG]  Creating dataset with 8108 registers\n",
      "2021-02-15 17:07:00,236 [conabio_ml.datasets.dataset] [DEBUG]  Dataset with ordinal labels\n",
      "2021-02-15 17:07:00,237 [conabio_ml.datasets.dataset] [DEBUG]  2 categories in dataset\n",
      "2021-02-15 17:07:00,238 [conabio_ml.datasets.dataset] [INFO ]  Assigning labelmap with [{0: 'non_species'}, {1: 'species'}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>partition</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>dif iff ffe fer ere ren ent nt_ t_g _ge gen en...</td>\n",
       "      <td>non_species</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>hum uma man ans ns_ s_p _pi pig igs gs_ s_m _m...</td>\n",
       "      <td>non_species</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>29</td>\n",
       "      <td>sap apo pov ovi vir iru rus use ses</td>\n",
       "      <td>non_species</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>41</td>\n",
       "      <td>res esu sul ult lts ts_ s_o _of of_ f_a</td>\n",
       "      <td>non_species</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>52</td>\n",
       "      <td>hip ipp ppo pos osi sid ide der ero ros os_ s_...</td>\n",
       "      <td>non_species</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               item  \\\n",
       "15          16  dif iff ffe fer ere ren ent nt_ t_g _ge gen en...   \n",
       "22          24  hum uma man ans ns_ s_p _pi pig igs gs_ s_m _m...   \n",
       "26          29                sap apo pov ovi vir iru rus use ses   \n",
       "37          41            res esu sul ult lts ts_ s_o _of of_ f_a   \n",
       "48          52  hip ipp ppo pos osi sid ide der ero ros os_ s_...   \n",
       "\n",
       "          label partition     score  \n",
       "15  non_species      test  0.999482  \n",
       "22  non_species      test  0.999488  \n",
       "26  non_species      test  0.998485  \n",
       "37  non_species      test  0.998491  \n",
       "48  non_species      test  0.998266  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_dataset = trained_model.predict(\n",
    "    dataset=dataset,\n",
    "    execution_config=None,\n",
    "    prediction_config={\n",
    "        \"sparse_predictions\": False,\n",
    "        \"ommit_uniques\": True\n",
    "    }\n",
    ")\n",
    "predicted_dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluator.eval(dataset_true=dataset,\n",
    "                            dataset_pred=predicted_dataset,\n",
    "                            eval_config={\n",
    "                                \"dataset_partition\": Partitions.TEST,\n",
    "                                \"metrics_set\": {\n",
    "                                    Metrics.Sets.MULTICLASS: {\n",
    "                                        'per_class': True,\n",
    "                                        'average': 'macro',\n",
    "                                        \"zero_division\": 1.0\n",
    "                                    }\n",
    "                                }      \n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MULTICLASS': {'one_class': {'confusion_matrix': [[7051, 6], [1045, 6]],\n",
      "                              'f1_score': 0.4709648012349917,\n",
      "                              'labels': ['non_species', 'species'],\n",
      "                              'precision': 0.6854619565217391,\n",
      "                              'recall': 0.5024293145377177},\n",
      "                'per_class': {'non_species': {'f1_score': 0.9306407972018742,\n",
      "                                              'precision': 0.8709239130434783,\n",
      "                                              'recall': 0.9991497803599263},\n",
      "                              'species': {'f1_score': 0.011288805268109126,\n",
      "                                          'precision': 0.5,\n",
      "                                          'recall': 0.005708848715509039}}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(evaluation.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
