{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we perform the training process and the evaluation.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The process consists on the following stages:\n",
    "\n",
    "1. Load the dataset produced in the `eda` notebook.\n",
    "2. Generate n-char samples, then, add noise to the samples to simulate noisy process like OCR detection.\n",
    "3. Create the model, and the config model (in case you want to use `pipeline.py`), and train it.\n",
    "4. Make a predictions of the dataset and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to update the PYTHON_PATH to\n",
    "# export PYTHONPATH=`pwd`:`pwd`/conabio_ml_text/conabio_ml:`pwd`/conabio_ml_text\n",
    "\n",
    "from conabio_ml_text.datasets.dataset import Dataset, Partitions\n",
    "from conabio_ml_text.preprocessing.preprocessing import Tokens, PreProcessing\n",
    "from conabio_ml_text.preprocessing.transform import Transform\n",
    "\n",
    "from conabio_ml_text.trainers.bcknds.tfkeras import TFKerasTrainer, TFKerasTrainerConfig\n",
    "from conabio_ml_text.trainers.bcknds.tfkeras import CHECKPOINT_CALLBACK, TENSORBOARD_CALLBACK\n",
    "\n",
    "from conabio_ml_text.utils.constraints import TransformRepresentations as TR\n",
    "\n",
    "from conabio_ml.evaluator.generic.evaluator import Evaluator, Metrics\n",
    "\n",
    "from conabio_ml.utils.logger import get_logger, debugger\n",
    "\n",
    "from utils import nchars, datagen\n",
    "from model import LSTMModel\n",
    "\n",
    "log = get_logger(__name__)\n",
    "debug = debugger.debug\n",
    "\n",
    "Tokens.UNK_TOKEN = Tokens.UNK_TOKEN * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable this to see the `debug` messages\n",
    "# debugger.create(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = Path(\"dataset/dataset.csv\")\n",
    "results_path = Path(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset from the `csv` file and perform a split in `[train, validation, test]` partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv(dataset_filepath)\n",
    "dataset = Dataset.split(dataset,\n",
    "                        train_perc=0.8,\n",
    "                        test_perc=0.1,\n",
    "                        val_perc=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.get_partition(\"test\")), len(dataset.get_partition(\"test\")[\"item\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training process, based on a `char-rnn`, we create samples of n-chars to capture insights.\n",
    "\n",
    "Note we use the property `build_vocab=True` to create the vocabulary accordding to the resulting dataset. Also, the `func_args` parameters are sent to the preprocess function, defined in `preprocess_fn=nchars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBCHAR_SIZE = 2\n",
    "MAX_SAMPLE_SIZE = np.max(dataset.data[\"item\"].apply(lambda x: len(x)))\n",
    "SAMPLE_SIZE = MAX_SAMPLE_SIZE - (SUBCHAR_SIZE + 1)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset = PreProcessing.preprocess(dataset,\n",
    "                                   build_vocab=True,\n",
    "                                   preprocess_args={\n",
    "                                       \"fields\": [\"item\"],\n",
    "                                       \"func_args\": {\n",
    "                                           \"pad_size\": -1,\n",
    "                                           \"nchar_size\": SUBCHAR_SIZE,\n",
    "                                           \"unk_token\": Tokens.UNK_TOKEN\n",
    "                                       }\n",
    "                                   },\n",
    "                                   preprocess_fn=nchars)\n",
    "# # By this moment the dataset is already processed and the vocab property exists\n",
    "# # Taken from TransformRepresentations constraints\n",
    "pprint(dataset.representations[TR.VOCAB][0:5])\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add a noise process. \n",
    "\n",
    "We produce 3 new noise samples from 1 real sample, 4 in total. With the following methodology:\n",
    "1. Real sample [nchar_1, nchar_2, â€¦, nchar_N, PAD_TOKEN]\n",
    "2. Two samples replacing 1 {nchar_1, nchar_N} to UNK_TOKEN\n",
    "3. One sample with both previously replaced `nchars``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Transform.as_data_generator(dataset,\n",
    "                                      vocab=None,\n",
    "                                      transform_args={\n",
    "                                          \"pad_length\": SAMPLE_SIZE,\n",
    "                                          \"unk_token\": Tokens.UNK_TOKEN,\n",
    "                                          \"batch_size\": BATCH_SIZE\n",
    "                                      },\n",
    "                                      data_generator=datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Be aware with batch size in model training, we are using the current batch_size in \n",
    "## `as_data_generator`, because it will produce samples of `[batch_size X 4 X max_len] (3D)`\n",
    "## instead of `[(batch_size X 4) X max_len] (2D)`\n",
    "#vocab = dataset.representations[TR.VOCAB]\n",
    "#gen = dataset.representations[\"data_generators\"][\"train\"]\n",
    "#sample_x, sample_y = next(gen())\n",
    "#[print([vocab[char] for char in sample]) for sample in sample_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trainer_config` contains parameters of the environment where the model will be trained. Like callbacks, and strategies (if there is one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REPRESENTATION = TR.DATA_GENERATORS\n",
    "trainer_config = TFKerasTrainerConfig.create(config={\n",
    "        \"strategy\": None,\n",
    "        \"callbacks\": {\n",
    "            CHECKPOINT_CALLBACK: {\n",
    "                \"filepath\": os.path.join(results_path, \"checkpoints\"),\n",
    "                \"save_best_only\": False\n",
    "            },\n",
    "            TENSORBOARD_CALLBACK: {\n",
    "                \"log_dir\": os.path.join(results_path, \"tb_logs\")\n",
    "            }}\n",
    "    })\n",
    "VOCAB_SIZE = len(dataset.representations[TR.VOCAB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a template of the `config`, in case you want to use it in `pipeline.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": str(\"code\" / dataset_filepath),\n",
    "    \"layers\": {\n",
    "        \"input\": {\n",
    "            \"T\": int(SAMPLE_SIZE)\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"V\": VOCAB_SIZE,\n",
    "            \"D\": 200\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"M\": 48,\n",
    "            \"dropout\":0.6,\n",
    "            \"recurrent_dropout\":0.6\n",
    "        },\n",
    "        \"dense\": {\n",
    "            \"K\": 2\n",
    "        }\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"initial_learning_rate\": 0.0002,\n",
    "        \"decay_steps\": 5000,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/config.json\", mode=\"w\") as _f:\n",
    "    json.dump(config, _f, indent=4)\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Model definition.<br> \n",
    "<i>In case you change the model (or create a new one), be sure the config of the layers match with your own definition, in <code>LSTMModel.create_model</code>.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = config[\"layers\"]\n",
    "model = LSTMModel.create(model_config={        \n",
    "    \"CLASSIFIER\": {\n",
    "        \"layers\": {\n",
    "            \"input\": layers[\"input\"],\n",
    "            \"embedding\": layers[\"embedding\"],\n",
    "            \"lstm\": layers[\"lstm\"],\n",
    "            \"dense\": layers[\"dense\"]\n",
    "        }\n",
    "    }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all keyparts: `dataset`, `model`, `trainer_config`, to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config[\"params\"][\"initial_learning_rate\"]\n",
    "epochs = config[\"params\"][\"epochs\"]\n",
    "trained_model = TFKerasTrainer.train(dataset=dataset,\n",
    "                                     model=model,\n",
    "                                     execution_config=trainer_config,\n",
    "                                     train_config={\n",
    "                                         \"CLASSIFIER\": {\n",
    "                                             \"representation\": TRAIN_REPRESENTATION,\n",
    "                                             'optimizer': tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                                             'loss': tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                             \"batch_size\": None,\n",
    "                                             \"epochs\": epochs,\n",
    "                                             \"metrics\": [\"accuracy\"]\n",
    "                                         }\n",
    "                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this complete process in a sequential pipeline in the script `pipeline.py`, with:\n",
    "```shell\n",
    "python pipeline.py -c path/to/config_file.json\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
